apiVersion: v1
kind: ConfigMap
metadata:
  name: oap-config
  namespace: {{.Release.Namespace}}
data:
  envoy.yaml: |
    admin:
      access_log_path: /tmp/admin_access.log
      address:
        socket_address: { address: 0.0.0.0, port_value: 50001 }
    static_resources:
      listeners:
      - name: es_listener
        address:
          socket_address: { address: 0.0.0.0, port_value: 9200 }
        filter_chains:
        - filters:
          - name: envoy.http_connection_manager
            config:
              stat_prefix: elasticsearch
              codec_type: AUTO
              route_config:
                name: local_route
                request_headers_to_add:
                - header:
                    key: tcc-route-target
                    value: elasticsearch
                {{- if .Values.token }}
                - header:
                    key: Authorization
                    value: "Bearer $token"
                {{- end }}
                virtual_hosts:
                - name: local_service
                  domains: ['*']
                  routes:
                  - match: { prefix: '/' }
                    route: { cluster: elasticsearch }
              http_filters:
              - name: envoy.router
      - name: oap_tcc_listener
        address:
          socket_address: { address: 0.0.0.0, port_value: 21800 }
        filter_chains:
        - filters:
          - name: envoy.http_connection_manager
            config:
              stat_prefix: oap_tcc
              codec_type: AUTO
              route_config:
                name: local_route
                request_headers_to_add:
                {{- if .Values.token }}
                - header:
                    key: Authorization
                    value: "Bearer $token"
                {{- end }}
                virtual_hosts:
                - name: local_service
                  domains: ['*']
                  routes:
                  - match: { prefix: '/' }
                    route: { cluster: oap-tcc }
              http_filters:
              - name: envoy.router
      clusters:
      - name: elasticsearch
        connect_timeout: 5s
        type: STRICT_DNS
        lb_policy: ROUND_ROBIN
        tls_context: {}
        hosts:
        - socket_address:
            address: {{ .Values.elasticsearch.host }}
            port_value: {{ .Values.elasticsearch.port }}
      - name: oap-tcc
        connect_timeout: 5s
        type: STRICT_DNS
        lb_policy: ROUND_ROBIN
        tls_context: {}
        http2_protocol_options: {}
        hosts:
        - socket_address:
            address: {{ .Values.global.tcc.host }}
            port_value: {{ .Values.global.tcc.port }}
  application.yml: |-
    cluster:
      kubernetes:
        watchTimeoutSeconds: 60
        namespace: {{.Release.Namespace}}
        labelSelector: app=oap
        uidEnvName: SKYWALKING_COLLECTOR_UID
    core:
      default:
        # Mixed: Receive agent data, Level 1 aggregate, Level 2 aggregate
        # Receiver: Receive agent data, Level 1 aggregate
        # Aggregator: Level 2 aggregate
        role: ${SW_CORE_ROLE:Mixed} # Mixed/Receiver/Aggregator
        restHost: ${SW_CORE_REST_HOST:0.0.0.0}
        restPort: ${SW_CORE_REST_PORT:12800}
        restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/}
        gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0}
        gRPCPort: ${SW_CORE_GRPC_PORT:11800}
        downsampling:
          - Hour
          - Day
        persistentPeriod: 10
        # Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted.
        # User zone SPM doesn't do delete, TCC SPM do that centrally.
        enableDataKeeperExecutor: ${SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR:false} # Turn it off then automatically metrics data delete will be close.
        recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:90} # Unit is minute
        minuteMetricsDataTTL: ${SW_CORE_MINUTE_METRIC_DATA_TTL:90} # Unit is minute
        hourMetricsDataTTL: ${SW_CORE_HOUR_METRIC_DATA_TTL:36} # Unit is hour
        dayMetricsDataTTL: ${SW_CORE_DAY_METRIC_DATA_TTL:45} # Unit is day
        monthMetricsDataTTL: ${SW_CORE_MONTH_METRIC_DATA_TTL:18} # Unit is month
    storage:
      elasticsearch:
        nameSpace: ${SW_NAMESPACE:""}
        clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200}
        user: ${SW_ES_USER:""}
        password: ${SW_ES_PASSWORD:""}
        indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2}
        indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0}
        # Those data TTL settings will override the same settings in core module.
        recordDataTTL: ${SW_STORAGE_ES_RECORD_DATA_TTL:7} # Unit is day
        otherMetricsDataTTL: ${SW_STORAGE_ES_OTHER_METRIC_DATA_TTL:45} # Unit is day
        monthMetricsDataTTL: ${SW_STORAGE_ES_MONTH_METRIC_DATA_TTL:18} # Unit is month
        # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html
        bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests
        flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests
        concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests
        metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000}
        segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200}
    receiver-sharing-server:
      default:
    receiver-register:
      default:
    service-mesh:
      default:
        bufferPath: ${SW_SERVICE_MESH_BUFFER_PATH:../mesh-buffer/}  # Path to trace buffer files, suggest to use absolute path
        bufferOffsetMaxFileSize: ${SW_SERVICE_MESH_OFFSET_MAX_FILE_SIZE:100} # Unit is MB
        bufferDataMaxFileSize: ${SW_SERVICE_MESH_BUFFER_DATA_MAX_FILE_SIZE:500} # Unit is MB
        bufferFileCleanWhenRestart: ${SW_SERVICE_MESH_BUFFER_FILE_CLEAN_WHEN_RESTART:false}
    spm-envoy-metric:
      default:
        enableMetricsService: ${SW_ENABLE_ENVOY_METRICS_SERIVCE:true}
        enableALS: ${SW_ENABLE_ENVOY_ALS:true}
        alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:reg-agent-mesh}
        cluster: {{ .Values.global.tcc.cluster }}
        env: {{ .Values.global.tcc.environment }}
        regAgentHost: ${SW_ENVOY_METRIC_ALS_REG_AGENT_HOST:regsource}
        regAgentPort: ${SW_ENVOY_METRIC_ALS_REG_AGENT_PORT:9080}
    query:
      graphql:
        path: ${SW_QUERY_GRAPHQL_PATH:/graphql}
    alarm:
      default:
    telemetry:
      prometheus:
        host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0}
        port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234}
    configuration:
      none:
    exporter:
      user2central:
        targetHost: ${SW_EXPORTER_GRPC_HOST:localhost}
        targetPort: ${SW_EXPORTER_GRPC_PORT:21800}
        bufferPath: ${SW_EXPORTER_BUFFER_PATH:../exporter-buffer/}  # Path to trace buffer files, suggest to use absolute path
        bufferOffsetMaxFileSize: ${SW_EXPORTER_BUFFER_OFFSET_MAX_FILE_SIZE:100} # Unit is MB
        bufferDataMaxFileSize: ${SW_EXPORTER_BUFFER_DATA_MAX_FILE_SIZE:500} # Unit is MB
        bufferFileCleanWhenRestart: ${SW_EXPORTER_BUFFER_FILE_CLEAN_WHEN_RESTART:false}

  log4j2.xml: |-
    <Configuration status="WARN">
        <Appenders>
            <Console name="Console" target="SYSTEM_OUT">
                <PatternLayout charset="UTF-8" pattern="%d - %c -%-4r [%t] %-5p %x - %m%n"/>
            </Console>
        </Appenders>
        <Loggers>
            <logger name="org.eclipse.jetty" level="INFO"/>
            <logger name="org.apache.zookeeper" level="INFO"/>
            <logger name="org.elasticsearch.common.network.IfConfig" level="INFO"/>
            <logger name="io.grpc.netty" level="INFO"/>
            <logger name="org.apache.skywalking.oap.server.receiver.istio.telemetry" level="INFO"/>
            <Root level="WARN">
                <AppenderRef ref="Console"/>
            </Root>
        </Loggers>
    </Configuration>

  official_analysis.oal: |-

    // Service scope metric
    service_heatmap = from(Service.latency).thermodynamic(100, 20);
    service_resp_time = from(Service.latency).longAvg();
    service_sla = from(Service.*).percent(status == true);
    service_cpm = from(Service.*).cpm();

    service_p99 = from(Service.latency).p99(10);
    service_p95 = from(Service.latency).p95(10);
    service_p90 = from(Service.latency).p90(10);
    service_p75 = from(Service.latency).p75(10);
    service_p50 = from(Service.latency).p50(10);

    service_2xx = from(Service.*).filter(responseCode >= 200).filter(responseCode < 400).cpm();
    service_4xx = from(Service.*).filter(responseCode >= 400).filter(responseCode < 500).cpm();
    service_5xx = from(Service.*).filter(responseCode >= 500).cpm();

    // Service Instance Scope metric
    service_instance_sla = from(ServiceInstance.*).percent(status == true);
    service_instance_resp_time= from(ServiceInstance.latency).longAvg();
    service_instance_cpm = from(ServiceInstance.*).cpm();

    service_instance_p99 = from(ServiceInstance.latency).p99(10);
    service_instance_p95 = from(ServiceInstance.latency).p95(10);
    service_instance_p90 = from(ServiceInstance.latency).p90(10);
    service_instance_p75 = from(ServiceInstance.latency).p75(10);
    service_instance_p50 = from(ServiceInstance.latency).p50(10);

    service_instance_2xx = from(ServiceInstance.*).filter(responseCode >= 200).filter(responseCode < 400).cpm();
    service_instance_4xx = from(ServiceInstance.*).filter(responseCode >= 400).filter(responseCode < 500).cpm();
    service_instance_5xx = from(ServiceInstance.*).filter(responseCode >= 500).cpm();

    // Envoy instance metric
    envoy_heap_memory_max_used = from(EnvoyInstanceMetric.value).filter(metricName == "server.memory_heap_size").maxDouble();
    envoy_total_connections_used = from(EnvoyInstanceMetric.value).filter(metricName == "server.total_connections").maxDouble();
    envoy_parent_connections_used = from(EnvoyInstanceMetric.value).filter(metricName == "server.parent_connections").maxDouble();

    // Service relation scope metric for topology
    service_relation_client_cpm = from(ServiceRelation.*).filter(detectPoint == DetectPoint.CLIENT).cpm();
    service_relation_server_cpm = from(ServiceRelation.*).filter(detectPoint == DetectPoint.SERVER).cpm();
    service_relation_client_call_sla = from(ServiceRelation.*).filter(detectPoint == DetectPoint.CLIENT).percent(status == true);
    service_relation_server_call_sla = from(ServiceRelation.*).filter(detectPoint == DetectPoint.SERVER).percent(status == true);
    service_relation_client_resp_time = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.CLIENT).longAvg();
    service_relation_server_resp_time = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.SERVER).longAvg();

    // Endpoint scope metric
    endpoint_cpm = from(Endpoint.*).cpm();
    endpoint_avg = from(Endpoint.latency).longAvg();
    endpoint_sla = from(Endpoint.*).percent(status == true);
    endpoint_p99 = from(Endpoint.latency).p99(10);
    endpoint_p95 = from(Endpoint.latency).p95(10);
    endpoint_p90 = from(Endpoint.latency).p90(10);
    endpoint_p75 = from(Endpoint.latency).p75(10);
    endpoint_p50 = from(Endpoint.latency).p50(10);

    endpoint_2xx = from(Endpoint.*).filter(responseCode >= 200).filter(responseCode < 400).cpm();
    endpoint_4xx = from(Endpoint.*).filter(responseCode >= 400).filter(responseCode < 500).cpm();
    endpoint_5xx = from(Endpoint.*).filter(responseCode >= 500).cpm();

    // Disable unnecessary hard core sources
    /////////
    // disable(service_relation_server_side);
    // disable(service_relation_client_side);
    disable(segment);
    disable(endpoint_relation_server_side);
    disable(top_n_database_statement);
    disable(zipkin_span);
    disable(jaeger_span);
